{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82161688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV shape: (335, 30)\n",
      "🚀 Starting Processing\n",
      "Total rows: 335 | Batch size: 50 | Total batches: 7\n",
      "Initial memory usage: 404.28 MB\n",
      "------------------------------------------------------------\n",
      "\n",
      " Batch 1/7 (Rows 0-49)\n",
      "Memory before batch: 404.31 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 1:  68%|██████▊   | 34/50 [00:22<00:11,  1.44it/s]"
     ]
    }
   ],
   "source": [
    "#Typical Libraries\n",
    "\n",
    "import pandas as pd #Pandas for DataFrame's\n",
    "import numpy as np #Numpy for math\n",
    "from PIL import Image #Pillow for image processing\n",
    "from pathlib import Path #Pathlib as an OS replacement for paths\n",
    "import matplotlib.pyplot as plt #Matplotlib for plotting data\n",
    "\n",
    "#Tracking progress and time\n",
    "\n",
    "import time\n",
    "from datetime import datetime #To get the current time for a timestamp\n",
    "from zoneinfo import ZoneInfo #To set my timezone\n",
    "from tqdm import tqdm #tdqm for progress bars\n",
    "\n",
    "#Possible Viz Enhancements\n",
    "\n",
    "import seaborn as sns #Seaborn for advanced plotting (tbd)\n",
    "from tabulate import tabulate #Tabulate for pretty tables (tbd)\n",
    "\n",
    "#Machine Learning with Sci-Kit Learn, TensorFlow, and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "#CNN Preprocessing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #For data augmentation\n",
    "\n",
    "#Constants for the model\n",
    "IMAGE_SIZE = (128, 128)\n",
    "DATASET_PATH = Path(\"../Data/data_images\")\n",
    "CSV_PATH = Path(\"../Data/data_sheet.csv\")\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"CSV shape:\", df.shape)\n",
    "df.head()\n",
    "\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def load_and_match_images(image_path, size=IMAGE_SIZE):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize(size)\n",
    "    return np.array(image) / 255.0 # Normalize to [0, 1]\n",
    "\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_mb = process.memory_info().rss / 1024**2\n",
    "    return f\"{mem_mb:.2f} MB\"\n",
    "\n",
    "# Adaptive garbage collection if memory exceeds threshold\n",
    "def adaptive_gc(threshold_gb=35):\n",
    "    mem_gb = psutil.Process(os.getpid()).memory_info().rss / 1024**3\n",
    "    if mem_gb > threshold_gb:\n",
    "        print(f\"RAM at {mem_gb:.2f} GB — triggering garbage collection\")\n",
    "        gc.collect()\n",
    "\n",
    "def process_large_folders(df, dataset_path, batch_size=1000, memory_threshold_gb=35):\n",
    "    matched_data = []\n",
    "    start = time.time()\n",
    "    total_batches = (len(df) - 1) // batch_size + 1\n",
    "\n",
    "    print(f\"🚀 Starting Processing\")\n",
    "    print(f\"Total rows: {len(df)} | Batch size: {batch_size} | Total batches: {total_batches}\")\n",
    "    print(f\"Initial memory usage: {get_memory_usage()}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for batch_start in range(0, len(df), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(df))\n",
    "        batch_df = df.iloc[batch_start:batch_end]\n",
    "\n",
    "        batch_num = batch_start // batch_size + 1\n",
    "        print(f\"\\n Batch {batch_num}/{total_batches} (Rows {batch_start}-{batch_end-1})\")\n",
    "        print(f\"Memory before batch: {get_memory_usage()}\")\n",
    "\n",
    "        batch_data = []\n",
    "        for idx, (_, row) in enumerate(tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Processing Batch {batch_num}\")):\n",
    "            folder_name = str(row['dummy_id']) + \"_ct_images\"\n",
    "            folder_path = dataset_path / folder_name\n",
    "\n",
    "            if folder_path.exists():\n",
    "                image_files = sorted(folder_path.glob(\"*.jpg\"))\n",
    "                image_batch = []\n",
    "\n",
    "                for j, img_path in enumerate(image_files):\n",
    "                    img = load_and_match_images(img_path)\n",
    "                    image_batch.append(img)\n",
    "\n",
    "                    if (j + 1) % 1000 == 0:\n",
    "                        adaptive_gc(threshold_gb=memory_threshold_gb)\n",
    "\n",
    "                if image_batch:\n",
    "                    batch_data.append((row, image_batch.copy()))\n",
    "                    del image_batch\n",
    "                    gc.collect()\n",
    "            else:\n",
    "                if idx < 5:\n",
    "                    print(f\"  ⚠️ Missing folder: {folder_path}\")\n",
    "\n",
    "        matched_data.extend(batch_data)\n",
    "        del batch_data\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"Batch {batch_num} complete | Memory now: {get_memory_usage()} | Total matched: {len(matched_data)}\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"\\nPROCESSING COMPLETE\")\n",
    "    print(f\"Total time: {end - start:.2f}s | Avg per batch: {(end - start)/total_batches:.2f}s\")\n",
    "    print(f\"Final memory usage: {get_memory_usage()} | Total matched folders: {len(matched_data)}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return matched_data\n",
    "# Call the function to start the processing\n",
    "matched_data = process_large_folders(df, DATASET_PATH, batch_size=50)\n",
    "\n",
    "\n",
    "# Filter only L/R labels and map to binary\n",
    "binary_df = df[df[\"Tumor laterality\"].isin([\"L\", \"R\"])].copy()\n",
    "binary_df[\"Binary Label\"] = binary_df[\"Tumor laterality\"].map({\"L\": 0, \"R\": 1})\n",
    "\n",
    "print(\"Preview of binary_df:\")\n",
    "print(binary_df[[\"dummy_id\", \"Tumor laterality\", \"Binary Label\"]].head())\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Ensure binary_df is indexed by dummy_id for fast lookup\n",
    "binary_df.set_index(\"dummy_id\", inplace=True)\n",
    "\n",
    "for row, images in matched_data:\n",
    "    dummy_id = row[\"dummy_id\"]\n",
    "\n",
    "    # Skip any samples not labeled as L/R (already filtered in binary_df)\n",
    "    if dummy_id in binary_df.index:\n",
    "        label = binary_df.loc[dummy_id, \"Binary Label\"]\n",
    "        \n",
    "        # You can change this to any image reduction strategy you prefer\n",
    "        avg_image = np.mean(images, axis=0)  # Shape: (H, W) or (H, W, 1)\n",
    "        \n",
    "        X.append(avg_image)\n",
    "        y.append(label)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Optional: Expand dims if grayscale\n",
    "if len(X.shape) == 3:  # (samples, height, width)\n",
    "    X = X[..., np.newaxis]  # → (samples, height, width, 1)\n",
    "\n",
    "#Train-Test Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "#Data Augmentation for CT Scans\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "#Reading CSV to Print DataSet\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print()\n",
    "print(\"-\"*60)\n",
    "print(\"Dataframe shape:\", df.shape)\n",
    "print(\"-\"*60)\n",
    "print(\"Selected Parameter:\")\n",
    "print()\n",
    "df[\"Tumor laterality\"] = df[\"Tumor laterality\"].str.strip().str.upper()\n",
    "laterality_counts = df[\"Tumor laterality\"].value_counts(dropna=False)\n",
    "print(laterality_counts.to_string())\n",
    "print()\n",
    "print(\"-\"*60)\n",
    "print()\n",
    "\n",
    "#CNN Model Defined\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(2)\n",
    "])\n",
    "\n",
    "#Model Compilation with ADAM\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Model Summary Print\n",
    "model.summary()\n",
    "\n",
    "#How to Train Your Dragon (or model)\n",
    "history = model.fit(X_train, y_train, epochs=15, \n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "# Evaluate The Model w/ Test Data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"\\nTest accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "# Plot training & validation\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('cnn_accuracy_plot.png') \n",
    "plt.show()\n",
    "\n",
    "model.save('HNSCC_CNN_Model.h5')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
